{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (4.19.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence-transformers in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from sentence-transformers) (4.39.2)\n",
      "Requirement already satisfied: tqdm in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from sentence-transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium\n",
    "%pip install pandas\n",
    "%pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "from time import sleep\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Alexander Alberro, Title: , URL: https://arthistory.columbia.edu/content/alexander-alberro\n",
      "Name: Zainab Bahrani, Title: , URL: https://arthistory.columbia.edu/content/zainab-bahrani\n",
      "Name: Frédérique Baumgartner, Title: , URL: https://arthistory.columbia.edu/content/frederique-baumgartner\n",
      "Name: Barry Bergdoll, Title: , URL: https://arthistory.columbia.edu/content/barry-bergdoll\n",
      "Name: Diane Bodart, Title: , URL: https://arthistory.columbia.edu/content/diane-bodart\n",
      "Name: Julia Bryan-Wilson, Title: , URL: https://arthistory.columbia.edu/content/julia-bryan-wilson\n",
      "Name: Gregory Bryda, Title: , URL: https://arthistory.columbia.edu/content/gregory-bryda\n",
      "Name: Zeynep Çelik Alexander, Title: , URL: https://arthistory.columbia.edu/content/zeynep-celik-alexander\n",
      "Name: Michael Cole, Title: , URL: https://arthistory.columbia.edu/content/michael-cole\n",
      "Name: Jonathan K. Crary, Title: , URL: https://arthistory.columbia.edu/content/jonathan-k-crary\n",
      "Name: Francesco de Angelis, Title: , URL: https://arthistory.columbia.edu/content/francesco-de-angelis\n",
      "Name: Rosalyn Deutsche, Title: , URL: https://arthistory.columbia.edu/content/rosalyn-deutsche\n",
      "Name: Noam M. Elcott, Title: , URL: https://arthistory.columbia.edu/content/noam-m-elcott\n",
      "Name: David Freedberg, Title: , URL: https://arthistory.columbia.edu/content/david-freedberg\n",
      "Name: Meredith Gamer, Title: , URL: https://arthistory.columbia.edu/content/meredith-gamer\n",
      "Name: Anne Higonnet, Title: , URL: https://arthistory.columbia.edu/content/anne-higonnet\n",
      "Name: Elizabeth W. Hutchinson, Title: , URL: https://arthistory.columbia.edu/content/elizabeth-w-hutchinson\n",
      "Name: Kellie Jones, Title: , URL: https://arthistory.columbia.edu/content/kellie-jones\n",
      "Name: Branden W. Joseph, Title: , URL: https://arthistory.columbia.edu/content/branden-w-joseph\n",
      "Name: Subhashini Kaligotla, Title: , URL: https://arthistory.columbia.edu/content/subhashini-kaligotla\n",
      "Name: Holger A. Klein, Title: , URL: https://arthistory.columbia.edu/content/holger-klein\n",
      "Name: Rosalind Krauss, Title: , URL: https://arthistory.columbia.edu/content/rosalind-krauss\n",
      "Name: Janet Kraynak, Title: , URL: https://arthistory.columbia.edu/content/janet-kraynak\n",
      "Name: Matthew Philip McKelway, Title: , URL: https://arthistory.columbia.edu/content/matthew-philip-mckelway\n",
      "Name: Ioannis Mylonopoulos, Title: , URL: https://arthistory.columbia.edu/content/ioannis-mylonopoulos\n",
      "Name: Eleonora Pistis, Title: , URL: https://arthistory.columbia.edu/content/eleonora-pistis\n",
      "Name: Jonathan Reynolds, Title: , URL: https://arthistory.columbia.edu/content/jonathan-reynolds\n",
      "Name: Simon Schama, Title: , URL: https://arthistory.columbia.edu/content/simon-schama\n",
      "Name: Avinoam Shalem, Title: , URL: https://arthistory.columbia.edu/content/avinoam-shalem\n",
      "Name: Z. S. Strother, Title: , URL: https://arthistory.columbia.edu/content/z-s-strother\n",
      "Name: Lisa Trever, Title: , URL: https://arthistory.columbia.edu/content/lisa-trever\n",
      "Name: Michael J. Waters, Title: , URL: https://arthistory.columbia.edu/content/michael-j-waters\n",
      "Name: , Title: , URL: https://arthistory.columbia.edu/content/jin-xu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to extract biography from https://arthistory.columbia.edu/content/rosalyn-deutsche: list index out of range\n"
     ]
    }
   ],
   "source": [
    "def scrape_directory(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    names = []\n",
    "    bio_urls = []\n",
    "    titles = []\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        while True:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"p\"))\n",
    "            )\n",
    "\n",
    "            faculty_listings = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "            for faculty in faculty_listings:\n",
    "                try:\n",
    "                    link_element = faculty.find_element(By.TAG_NAME, \"a\")\n",
    "\n",
    "                    name = link_element.get_attribute('title')\n",
    "\n",
    "                    title_div = faculty.find_element(By.TAG_NAME, \"br\")\n",
    "                    title = title_div.text\n",
    "\n",
    "                    link = link_element.get_attribute('href')\n",
    "                    bio_url = urljoin(url, link)\n",
    "\n",
    "                    names.append(name)\n",
    "                    titles.append(title)\n",
    "                    bio_urls.append(bio_url)\n",
    "                    print(f\"Name: {name}, Title: {title}, URL: {bio_url}\")\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "\n",
    "            try:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                next_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.PARTIAL_LINK_TEXT, \"Next\"))\n",
    "                )\n",
    "\n",
    "                retries = 5\n",
    "                for _ in range(retries):\n",
    "                    try:\n",
    "                        next_button.click()\n",
    "                        break\n",
    "                    except ElementClickInterceptedException:\n",
    "                        sleep(1)\n",
    "                else:\n",
    "                    print(\"Failed to click the 'Next' button after several retries.\")\n",
    "                    break\n",
    "\n",
    "            except (TimeoutException, NoSuchElementException):\n",
    "                # print(\"No 'Next' button found, or it's not clickable.\")\n",
    "                break\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Timeout while waiting for page to load\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    df = pd.DataFrame({'Name': names, 'Title': titles, 'url': bio_urls})\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_biographies(df):\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Create a new column 'Research Summary'\n",
    "    df['Research Summary'] = None  # Initialize with None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        bio_url = row['url']\n",
    "        try:\n",
    "            driver.get(bio_url)\n",
    "            full_bio_text = \"\"\n",
    "\n",
    "            # Find the original target section and extract text\n",
    "            try:\n",
    "                target_sections = driver.find_elements(By.CLASS_NAME, 'field--name-field-cu-wysiwyg')\n",
    "                target_section = target_sections[2]  # Get the second element\n",
    "                full_bio_text += target_section.text\n",
    "            except NoSuchElementException:\n",
    "                # If the target section is not found, continue without raising an error\n",
    "                pass\n",
    "\n",
    "            # Assign the 'Research Summary' to the corresponding row in the DataFrame\n",
    "            if full_bio_text:\n",
    "                df.at[index, 'Research Summary'] = full_bio_text\n",
    "            else:\n",
    "                df.at[index, 'Research Summary'] = None\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout while loading {bio_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract biography from {bio_url}: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    return df\n",
    "\n",
    "url = \"https://arthistory.columbia.edu/content/full-time-faculty\"\n",
    "df = scrape_directory(url)\n",
    "df = extract_biographies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentence embedder model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "# Assuming 'Research Summary' column contains the text data\n",
    "df['Research Embedding'] = df['Research Summary'].apply(lambda x: model.encode([x])[0] if x else None)\n",
    "gdf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = df\n",
    "# Assuming 'gdf' is your original DataFrame\n",
    "\n",
    "# Convert the 'Research Embedding' column to a list of lists\n",
    "gdf['Research Embedding'] = gdf['Research Embedding'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else None)\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "data_list = gdf.to_dict(orient='records')\n",
    "\n",
    "# Save the list of dictionaries to a JSON file\n",
    "with open('faculty_data_art.json', 'w') as json_file:\n",
    "    json.dump(data_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>url</th>\n",
       "      <th>Research Summary</th>\n",
       "      <th>Research Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexander Alberro</td>\n",
       "      <td></td>\n",
       "      <td>https://arthistory.columbia.edu/content/alexan...</td>\n",
       "      <td>Alexander Alberro’s courses and graduate advis...</td>\n",
       "      <td>[0.04002338647842407, 0.033464401960372925, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zainab Bahrani</td>\n",
       "      <td></td>\n",
       "      <td>https://arthistory.columbia.edu/content/zainab...</td>\n",
       "      <td>Zainab Bahrani is the Edith Porada Professor a...</td>\n",
       "      <td>[0.005627704318612814, 0.09003138542175293, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Frédérique Baumgartner</td>\n",
       "      <td></td>\n",
       "      <td>https://arthistory.columbia.edu/content/freder...</td>\n",
       "      <td>Frédérique Baumgartner’s research focuses on 1...</td>\n",
       "      <td>[-0.00390178756788373, 0.09777113795280457, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barry Bergdoll</td>\n",
       "      <td></td>\n",
       "      <td>https://arthistory.columbia.edu/content/barry-...</td>\n",
       "      <td>Professor Bergdoll's broad interests center on...</td>\n",
       "      <td>[0.04410383850336075, 0.12827448546886444, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diane Bodart</td>\n",
       "      <td></td>\n",
       "      <td>https://arthistory.columbia.edu/content/diane-...</td>\n",
       "      <td>Diane Bodart was educated in Art History at th...</td>\n",
       "      <td>[0.006267494987696409, -0.04520120471715927, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name Title  \\\n",
       "0       Alexander Alberro         \n",
       "1          Zainab Bahrani         \n",
       "2  Frédérique Baumgartner         \n",
       "3          Barry Bergdoll         \n",
       "4            Diane Bodart         \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://arthistory.columbia.edu/content/alexan...   \n",
       "1  https://arthistory.columbia.edu/content/zainab...   \n",
       "2  https://arthistory.columbia.edu/content/freder...   \n",
       "3  https://arthistory.columbia.edu/content/barry-...   \n",
       "4  https://arthistory.columbia.edu/content/diane-...   \n",
       "\n",
       "                                    Research Summary  \\\n",
       "0  Alexander Alberro’s courses and graduate advis...   \n",
       "1  Zainab Bahrani is the Edith Porada Professor a...   \n",
       "2  Frédérique Baumgartner’s research focuses on 1...   \n",
       "3  Professor Bergdoll's broad interests center on...   \n",
       "4  Diane Bodart was educated in Art History at th...   \n",
       "\n",
       "                                  Research Embedding  \n",
       "0  [0.04002338647842407, 0.033464401960372925, -0...  \n",
       "1  [0.005627704318612814, 0.09003138542175293, 0....  \n",
       "2  [-0.00390178756788373, 0.09777113795280457, -0...  \n",
       "3  [0.04410383850336075, 0.12827448546886444, 0.0...  \n",
       "4  [0.006267494987696409, -0.04520120471715927, 0...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON file back into a DataFrame\n",
    "with open('faculty_data_art.json', 'r') as json_file:\n",
    "    loaded_data_list = json.load(json_file)\n",
    "\n",
    "df = pd.DataFrame(loaded_data_list)\n",
    "\n",
    "# Now, gdf_loaded should have the 'Research Embedding' column in its original form\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "proposal = [\"\"\"\n",
    "medical image machine learning\n",
    "            \"\"\"]\n",
    "\n",
    "proposal_embedding = model.encode(proposal)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dot product for each row\n",
    "df['Dot Product'] = df['Research Embedding'].apply(lambda x: np.dot(proposal_embedding, x) if x is not None else None)\n",
    "\n",
    "# Sort DataFrame based on dot product scores in descending order\n",
    "df_sorted = df.sort_values(by='Dot Product', ascending=False)\n",
    "\n",
    "top_5 = df_sorted.head(5)\n",
    "\n",
    "print(top_5.iloc[0]['Research Summary'])\n",
    "\n",
    "top_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = proposal_embedding - top_5.iloc[0]['Research Embedding']\n",
    "# print(residual)\n",
    "proposal_embedding = residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = model.encode('king')\n",
    "norm = np.linalg.norm(king)\n",
    "king = king / norm\n",
    "\n",
    "man = model.encode('man')\n",
    "norm = np.linalg.norm(man)\n",
    "man = man / norm\n",
    "\n",
    "woman = model.encode('woman')\n",
    "norm = np.linalg.norm(woman)\n",
    "woman = woman / norm\n",
    "\n",
    "queen = model.encode('queen')\n",
    "norm = np.linalg.norm(queen)\n",
    "queen = queen / norm\n",
    "\n",
    "queen_test = king - man + woman\n",
    "norm = np.linalg.norm(queen_test)\n",
    "queen_test = queen_test / norm\n",
    "\n",
    "print('\\'king - man + woman\\' match to \\'king\\': ', np.dot(king, queen_test))\n",
    "print('\\'king - man + woman\\' match to \\'queen\\': ', np.dot(queen, queen_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dot product for each row\n",
    "df['Dot Product'] = df['Research Embedding'].apply(lambda x: np.dot(proposal_embedding, x) if x is not None else None)\n",
    "\n",
    "# Sort DataFrame based on dot product scores in descending order\n",
    "df_sorted = df.sort_values(by='Dot Product', ascending=False)\n",
    "\n",
    "top_5 = df_sorted.head(5)\n",
    "\n",
    "print(top_5.iloc[0]['Research Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Dot Product'])\n",
    "df.to_csv('faculty_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "def create_database():\n",
    "    conn = sqlite3.connect('biographies.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS biographies (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            url TEXT UNIQUE,\n",
    "            biography TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def insert_biography(url, biography):\n",
    "    conn = sqlite3.connect('biographies.db')\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"INSERT INTO biographies (url, biography) VALUES (?, ?)\", (url, biography))\n",
    "        conn.commit()\n",
    "    except sqlite3.IntegrityError:\n",
    "        print(f\"URL already exists in database: {url}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def extract_biographies(bio_urls):\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    for bio_url in bio_urls:\n",
    "        try:\n",
    "            driver.get(bio_url)\n",
    "            full_bio_text = \"\"\n",
    "\n",
    "            try:\n",
    "                body_section = driver.find_element(By.CLASS_NAME, 'body.col-lg-7')\n",
    "                full_bio_text += body_section.text + \" \"\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                target_section = driver.find_element(By.CLASS_NAME, 'col-lg-7.entity.entity-paragraphs-item.paragraphs-item-content')\n",
    "                full_bio_text += target_section.text\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "\n",
    "            if full_bio_text:\n",
    "                print(f\"Storing biography from {bio_url}\")\n",
    "                insert_biography(bio_url, full_bio_text)\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout while loading {bio_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract biography from {bio_url}: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# Create the database and table\n",
    "create_database()\n",
    "\n",
    "# Assuming you have a list of bio URLs\n",
    "bio_urls = [\"your_list_of_bio_urls_here\"]\n",
    "extract_biographies(bio_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
