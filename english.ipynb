{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in ./.venv/lib/python3.12/site-packages (4.19.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in ./.venv/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Requirement already satisfied: trio~=0.17 in ./.venv/lib/python3.12/site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in ./.venv/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in ./.venv/lib/python3.12/site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in ./.venv/lib/python3.12/site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in ./.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in ./.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in ./.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in ./.venv/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in ./.venv/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in ./.venv/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in ./.venv/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.39.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium\n",
    "%pip install pandas\n",
    "%pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolepi/Documents/Wing/find-the-expert/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
    "from time import sleep\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_directory(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    names = []\n",
    "    bio_urls = []\n",
    "    titles = []\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        while True:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"views-row\"))\n",
    "            )\n",
    "\n",
    "            faculty_listings = driver.find_elements(By.CLASS_NAME, \"views-row\")\n",
    "            for faculty in faculty_listings:\n",
    "                try:\n",
    "                    name = faculty.find_element(By.CSS_SELECTOR, \"h2 > span\").text\n",
    "                    title = faculty.find_element(By.CLASS_NAME, \"field--name-field-cu-title-department\").text\n",
    "                    article_element = faculty.find_element(By.TAG_NAME, \"article\")\n",
    "                    link_element = article_element.find_element(By.TAG_NAME, \"a\")\n",
    "                    link = link_element.get_attribute('href')\n",
    "                    bio_url = urljoin(url, link)\n",
    "                    names.append(name)\n",
    "                    titles.append(title)\n",
    "                    bio_urls.append(bio_url)\n",
    "                    #print(f\"Name: {name}, Title: {title}, URL: {bio_url}\")\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "\n",
    "            try:\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                next_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.PARTIAL_LINK_TEXT, \"Next\"))\n",
    "                )\n",
    "\n",
    "                retries = 5\n",
    "                for _ in range(retries):\n",
    "                    try:\n",
    "                        next_button.click()\n",
    "                        break\n",
    "                    except ElementClickInterceptedException:\n",
    "                        sleep(1)\n",
    "                else:\n",
    "                    print(\"Failed to click the 'Next' button after several retries.\")\n",
    "                    break\n",
    "\n",
    "            except (TimeoutException, NoSuchElementException):\n",
    "                # print(\"No 'Next' button found, or it's not clickable.\")\n",
    "                break\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Timeout while waiting for page to load\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    df = pd.DataFrame({'Name': names, 'Title': titles, 'url': bio_urls})\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_biographies(df):\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Create a new column 'Research Summary'\n",
    "    df['Research Summary'] = None  # Initialize with None\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        bio_url = row['url']\n",
    "        try:\n",
    "            driver.get(bio_url)\n",
    "            full_bio_text = \"\"\n",
    "\n",
    "            # Find the original target section and extract text\n",
    "            try:\n",
    "                target_sections = driver.find_elements(By.CLASS_NAME, 'field--name-field-cu-wysiwyg')\n",
    "                if len(target_sections) > 1:  # Check if there are at least two elements\n",
    "                    target_section = target_sections[1]  # Get the second element\n",
    "                    full_bio_text += target_section.text\n",
    "                else:\n",
    "                    pass\n",
    "            except NoSuchElementException:\n",
    "                # If the target section is not found, continue without raising an error\n",
    "                pass\n",
    "\n",
    "            # Assign the 'Research Summary' to the corresponding row in the DataFrame\n",
    "            if full_bio_text:\n",
    "                df.at[index, 'Research Summary'] = full_bio_text\n",
    "            else:\n",
    "                df.at[index, 'Research Summary'] = None\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout while loading {bio_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract biography from {bio_url}: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    return df\n",
    "\n",
    "\n",
    "url = \"https://english.columbia.edu/content/faculty\"\n",
    "df = scrape_directory(url)\n",
    "df = extract_biographies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentence embedder model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "\n",
    "# Assuming 'Research Summary' column contains the text data\n",
    "df['Research Embedding'] = df['Research Summary'].apply(lambda x: model.encode([x])[0] if x else None)\n",
    "gdf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = df\n",
    "# Assuming 'gdf' is your original DataFrame\n",
    "\n",
    "# Convert the 'Research Embedding' column to a list of lists\n",
    "gdf['Research Embedding'] = gdf['Research Embedding'].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else None)\n",
    "\n",
    "# Convert the DataFrame to a list of dictionaries\n",
    "data_list = gdf.to_dict(orient='records')\n",
    "\n",
    "# Save the list of dictionaries to a JSON file\n",
    "with open('faculty_data_english.json', 'w') as json_file:\n",
    "    json.dump(data_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Title</th>\n",
       "      <th>url</th>\n",
       "      <th>Research Summary</th>\n",
       "      <th>Research Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ERBIL ABACI</td>\n",
       "      <td>Assistant Professor at the Department of Derma...</td>\n",
       "      <td>https://www.engineering.columbia.edu/erbil-abaci</td>\n",
       "      <td>He has received his Ph.D. degree at the Johns ...</td>\n",
       "      <td>[-0.11000274121761322, 0.021928245201706886, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PABLO ABREU</td>\n",
       "      <td>Senior Program Manager</td>\n",
       "      <td>https://www.engineering.columbia.edu/pablo-abreu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ANISH AGARWAL</td>\n",
       "      <td>ASSISTANT PROFESSOR OF INDUSTRIAL ENGINEERING ...</td>\n",
       "      <td>https://www.engineering.columbia.edu/faculty/a...</td>\n",
       "      <td>Anish’s research interests are in designing an...</td>\n",
       "      <td>[-0.04620283097028732, 0.021099157631397247, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AJIT AGRAWAL</td>\n",
       "      <td>Adjunct Associate Professor</td>\n",
       "      <td>https://www.engineering.columbia.edu/ajit-agrawal</td>\n",
       "      <td>Ajit is the founder of AKAnomics Inc, a startu...</td>\n",
       "      <td>[-0.049910321831703186, -0.10780283808708191, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUNIL K. AGRAWAL</td>\n",
       "      <td>PROFESSOR OF MECHANICAL ENGINEERING AND PROFES...</td>\n",
       "      <td>https://www.engineering.columbia.edu/faculty/s...</td>\n",
       "      <td>Sunil K. Agrawal has developed a highly visibl...</td>\n",
       "      <td>[-0.07710743695497513, -0.12651987373828888, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name                                              Title  \\\n",
       "0       ERBIL ABACI  Assistant Professor at the Department of Derma...   \n",
       "1       PABLO ABREU                             Senior Program Manager   \n",
       "2     ANISH AGARWAL  ASSISTANT PROFESSOR OF INDUSTRIAL ENGINEERING ...   \n",
       "3      AJIT AGRAWAL                        Adjunct Associate Professor   \n",
       "4  SUNIL K. AGRAWAL  PROFESSOR OF MECHANICAL ENGINEERING AND PROFES...   \n",
       "\n",
       "                                                 url  \\\n",
       "0   https://www.engineering.columbia.edu/erbil-abaci   \n",
       "1   https://www.engineering.columbia.edu/pablo-abreu   \n",
       "2  https://www.engineering.columbia.edu/faculty/a...   \n",
       "3  https://www.engineering.columbia.edu/ajit-agrawal   \n",
       "4  https://www.engineering.columbia.edu/faculty/s...   \n",
       "\n",
       "                                    Research Summary  \\\n",
       "0  He has received his Ph.D. degree at the Johns ...   \n",
       "1                                               None   \n",
       "2  Anish’s research interests are in designing an...   \n",
       "3  Ajit is the founder of AKAnomics Inc, a startu...   \n",
       "4  Sunil K. Agrawal has developed a highly visibl...   \n",
       "\n",
       "                                  Research Embedding  \n",
       "0  [-0.11000274121761322, 0.021928245201706886, -...  \n",
       "1                                               None  \n",
       "2  [-0.04620283097028732, 0.021099157631397247, 0...  \n",
       "3  [-0.049910321831703186, -0.10780283808708191, ...  \n",
       "4  [-0.07710743695497513, -0.12651987373828888, 0...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON file back into a DataFrame\n",
    "with open('faculty_data_english.json', 'r') as json_file:\n",
    "    loaded_data_list = json.load(json_file)\n",
    "\n",
    "df = pd.DataFrame(loaded_data_list)\n",
    "\n",
    "# Now, gdf_loaded should have the 'Research Embedding' column in its original form\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "proposal = [\"\"\"\n",
    "medical image machine learning\n",
    "            \"\"\"]\n",
    "\n",
    "proposal_embedding = model.encode(proposal)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dot product for each row\n",
    "df['Dot Product'] = df['Research Embedding'].apply(lambda x: np.dot(proposal_embedding, x) if x is not None else None)\n",
    "\n",
    "# Sort DataFrame based on dot product scores in descending order\n",
    "df_sorted = df.sort_values(by='Dot Product', ascending=False)\n",
    "\n",
    "top_5 = df_sorted.head(5)\n",
    "\n",
    "print(top_5.iloc[0]['Research Summary'])\n",
    "\n",
    "top_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = proposal_embedding - top_5.iloc[0]['Research Embedding']\n",
    "# print(residual)\n",
    "proposal_embedding = residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = model.encode('king')\n",
    "norm = np.linalg.norm(king)\n",
    "king = king / norm\n",
    "\n",
    "man = model.encode('man')\n",
    "norm = np.linalg.norm(man)\n",
    "man = man / norm\n",
    "\n",
    "woman = model.encode('woman')\n",
    "norm = np.linalg.norm(woman)\n",
    "woman = woman / norm\n",
    "\n",
    "queen = model.encode('queen')\n",
    "norm = np.linalg.norm(queen)\n",
    "queen = queen / norm\n",
    "\n",
    "queen_test = king - man + woman\n",
    "norm = np.linalg.norm(queen_test)\n",
    "queen_test = queen_test / norm\n",
    "\n",
    "print('\\'king - man + woman\\' match to \\'king\\': ', np.dot(king, queen_test))\n",
    "print('\\'king - man + woman\\' match to \\'queen\\': ', np.dot(queen, queen_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dot product for each row\n",
    "df['Dot Product'] = df['Research Embedding'].apply(lambda x: np.dot(proposal_embedding, x) if x is not None else None)\n",
    "\n",
    "# Sort DataFrame based on dot product scores in descending order\n",
    "df_sorted = df.sort_values(by='Dot Product', ascending=False)\n",
    "\n",
    "top_5 = df_sorted.head(5)\n",
    "\n",
    "print(top_5.iloc[0]['Research Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Dot Product'])\n",
    "df.to_csv('faculty_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "def create_database():\n",
    "    conn = sqlite3.connect('biographies.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS biographies (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            url TEXT UNIQUE,\n",
    "            biography TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def insert_biography(url, biography):\n",
    "    conn = sqlite3.connect('biographies.db')\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"INSERT INTO biographies (url, biography) VALUES (?, ?)\", (url, biography))\n",
    "        conn.commit()\n",
    "    except sqlite3.IntegrityError:\n",
    "        print(f\"URL already exists in database: {url}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def extract_biographies(bio_urls):\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    for bio_url in bio_urls:\n",
    "        try:\n",
    "            driver.get(bio_url)\n",
    "            full_bio_text = \"\"\n",
    "\n",
    "            try:\n",
    "                body_section = driver.find_element(By.CLASS_NAME, 'body.col-lg-7')\n",
    "                full_bio_text += body_section.text + \" \"\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                target_section = driver.find_element(By.CLASS_NAME, 'col-lg-7.entity.entity-paragraphs-item.paragraphs-item-content')\n",
    "                full_bio_text += target_section.text\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "\n",
    "            if full_bio_text:\n",
    "                print(f\"Storing biography from {bio_url}\")\n",
    "                insert_biography(bio_url, full_bio_text)\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(f\"Timeout while loading {bio_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract biography from {bio_url}: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# Create the database and table\n",
    "create_database()\n",
    "\n",
    "# Assuming you have a list of bio URLs\n",
    "bio_urls = [\"your_list_of_bio_urls_here\"]\n",
    "extract_biographies(bio_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
